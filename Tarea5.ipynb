{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3s29_1OP75_"
      },
      "source": [
        "# TAREA 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5RnNdISiQe5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f2t1Aa0vwZFn",
        "outputId": "b7f07aaf-f590-478a-e301-f6f3b8a40a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Waiting for headers] [1 I\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Waiting for headers] [Con\u001b[0m\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Ign:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,071 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,425 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,654 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,601 kB]\n",
            "Fetched 16.6 MB in 3s (5,175 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "50 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68lK0zkYQMTj",
        "outputId": "b5b9063a-8999-4062-8106-77c9a22e338b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uA93uTUuawGB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import numbers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from typing import List, Union, Dict, Any, Tuple\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from matplotlib.figure import Figure\n",
        "from matplotlib.axes import Axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IAxSB2hNP76A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import setuptools\n",
        "import sys\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "import time\n",
        "from pyspark.sql.functions import col, to_date, concat, lpad, lit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6SyqaE_7P76B"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import ChiSqSelector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_DU5B8DgP76B"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.ml.feature import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uoVmQ_V4P76C"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "               .appName('ml') \\\n",
        "               .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "3QZZEyO-P76C"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('public_emdat_request.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "X6OgZWj0P76C"
      },
      "outputs": [],
      "source": [
        "def clean_column_names(df):\n",
        "    new_columns = [col.replace(\" \", \"_\").lower().strip() for col in df.columns]\n",
        "    df = df.toDF(*new_columns)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "Ry32TFNHP76C"
      },
      "outputs": [],
      "source": [
        "df = clean_column_names(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS2G9lwtP76C",
        "outputId": "7d1decad-42b4-4813-a1ee-4feb4970ad6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- historic: string (nullable = true)\n",
            " |-- disaster_group: string (nullable = true)\n",
            " |-- disaster_subgroup: string (nullable = true)\n",
            " |-- disaster_type: string (nullable = true)\n",
            " |-- disaster_subtype: string (nullable = true)\n",
            " |-- iso: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- subregion: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- magnitude: double (nullable = true)\n",
            " |-- start_year: integer (nullable = true)\n",
            " |-- start_month: integer (nullable = true)\n",
            " |-- start_day: integer (nullable = true)\n",
            " |-- end_year: integer (nullable = true)\n",
            " |-- end_month: integer (nullable = true)\n",
            " |-- end_day: integer (nullable = true)\n",
            " |-- total_deaths: integer (nullable = true)\n",
            " |-- total_affected: integer (nullable = true)\n",
            " |-- insured_damage_adjusted: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df= df.drop(\"disno\", \"external_ids\", \"origin\",\"associated_types\",\"ofda_bha_response\",\n",
        "            \"appeal\",\"declaration\",\"aid_contribution\",\"latitude\",\"longitude\",\"river_basin\",\n",
        "            \"injured\",\"affected\",\"homeless\",\"cpi\",\"classification\",\"entry_date\",\"last_update\",\"classification_key\",\n",
        "            \"magnitude_scale\",\"event_name\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uhpqa_ueP76D",
        "outputId": "be292011-e6ab-4e1b-e2c6-4070a6d06f52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------------+-----------------+-----------------+--------------------+---+--------------------+--------------------+--------+--------------------+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+\n",
            "|historic|disaster_group|disaster_subgroup|    disaster_type|    disaster_subtype|iso|             country|           subregion|  region|            location|magnitude|start_year|start_month|start_day|end_year|end_month|end_day|total_deaths|total_affected|insured_damage_adjusted|\n",
            "+--------+--------------+-----------------+-----------------+--------------------+---+--------------------+--------------------+--------+--------------------+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+\n",
            "|     Yes|       Natural|   Meteorological|            Storm|    Tropical cyclone|USA|United States of ...|    Northern America|Americas|   Galveston (Texas)|    220.0|      1900|          9|        8|    1900|        9|      8|        6000|          null|                   null|\n",
            "|     Yes|       Natural|     Hydrological|            Flood|     Flood (General)|JAM|             Jamaica|Latin America and...|Americas|         Saint James|     null|      1900|          1|        6|    1900|        1|      6|         300|          null|                   null|\n",
            "|     Yes|       Natural|       Biological|         Epidemic|       Viral disease|JAM|             Jamaica|Latin America and...|Americas|               Porus|     null|      1900|          1|       13|    1900|        1|     13|          30|          null|                   null|\n",
            "|     Yes|       Natural|      Geophysical|Volcanic activity|            Ash fall|JPN|               Japan|        Eastern Asia|    Asia|                null|     null|      1900|          7|        7|    1900|        7|      7|          30|          null|                   null|\n",
            "|     Yes|       Natural|      Geophysical|       Earthquake|     Ground movement|TUR|             T�rkiye|        Western Asia|    Asia|KARS,KARAKURT,KAG...|      5.9|      1900|          7|       12|    1900|        7|     12|         140|          null|                   null|\n",
            "|     Yes|       Natural|   Climatological|          Drought|             Drought|IND|               India|       Southern Asia|    Asia|              Bengal|     null|      1900|       null|     null|    1900|     null|   null|     1250000|          null|                   null|\n",
            "|     Yes|       Natural|   Climatological|          Drought|             Drought|CPV|          Cabo Verde|  Sub-Saharan Africa|  Africa|         Countrywide|     null|      1900|       null|     null|    1900|     null|   null|       11000|          null|                   null|\n",
            "|     Yes|       Natural|       Biological|         Epidemic|Infectious diseas...|UGA|              Uganda|  Sub-Saharan Africa|  Africa|          Nationwide|     null|      1901|       null|     null|    1901|     null|   null|      200000|          null|                   null|\n",
            "|     Yes|       Natural|      Geophysical|       Earthquake|             Tsunami|JPN|               Japan|        Eastern Asia|    Asia|             Sanriku|      7.9|      1901|          8|       10|    1901|        8|     10|          18|            24|                   null|\n",
            "|     Yes|       Natural|      Geophysical|Volcanic activity|            Ash fall|GTM|           Guatemala|Latin America and...|Americas|                null|     null|      1902|          4|        8|    1902|        4|      8|        1000|          null|                   null|\n",
            "|     Yes|       Natural|   Meteorological|            Storm|    Tropical cyclone|MMR|             Myanmar|  South-eastern Asia|    Asia|             Rangoon|     null|      1902|          5|        4|    1902|        5|      4|         600|          null|                   null|\n",
            "|     Yes|       Natural|      Geophysical|Volcanic activity|            Ash fall|MTQ|          Martinique|Latin America and...|Americas|                null|     null|      1902|          5|        8|    1902|        5|      8|       30000|          null|                   null|\n",
            "|     Yes|       Natural|      Geophysical|Volcanic activity|            Ash fall|VCT|Saint Vincent and...|Latin America and...|Americas|                null|     null|      1902|          5|        7|    1902|        5|      7|        1565|          null|                   null|\n",
            "|     Yes|       Natural|      Geophysical|Volcanic activity|            Ash fall|JPN|               Japan|        Eastern Asia|    Asia|           Torishima|     null|      1902|          8|     null|    1902|        8|   null|         125|          null|                   null|\n",
            "|     Yes|       Natural|      Geophysical|Volcanic activity|            Ash fall|GTM|           Guatemala|Latin America and...|Americas|                null|     null|      1902|         10|       24|    1902|       10|     24|        6000|          null|                   null|\n",
            "|     Yes|       Natural|      Geophysical|       Earthquake|     Ground movement|SUN|        Soviet Union|      Eastern Europe|  Europe|Andishan,Tashkent...|      6.4|      1902|         12|       16|    1902|       12|     16|        4562|        125112|                   null|\n",
            "|     Yes|       Natural|      Geophysical|       Earthquake|     Ground movement|GTM|           Guatemala|Latin America and...|Americas|Quezaltenango, Sa...|      7.5|      1902|          4|       18|    1902|        4|     18|        2000|          null|                   null|\n",
            "|     Yes|       Natural|      Geophysical|       Earthquake|     Ground movement|SUN|        Soviet Union|      Eastern Europe|  Europe|Semacha (Azerbaijan)|      6.9|      1902|          2|       13|    1902|        2|     13|          86|         17540|                   null|\n",
            "|     Yes|       Natural|      Geophysical|       Earthquake|     Ground movement|CHN|               China|        Eastern Asia|    Asia| XINJIANG, TURKESTAN|      7.7|      1902|          8|       22|    1902|        8|     22|        2500|          null|                   null|\n",
            "|     Yes|       Natural|     Hydrological|            Flood|     Flood (General)|USA|United States of ...|    Northern America|Americas|   Passaic, Delaware|     null|      1903|       null|     null|    1903|     null|   null|        null|          null|                   null|\n",
            "+--------+--------------+-----------------+-----------------+--------------------+---+--------------------+--------------------+--------+--------------------+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "W7apxVhVP76D"
      },
      "outputs": [],
      "source": [
        "def create_date_column(df, year_col, month_col, day_col, new_col_name):\n",
        "    df = df.withColumn(month_col, lpad(col(month_col), 2, \"0\")) \\\n",
        "           .withColumn(day_col, lpad(col(day_col), 2, \"0\"))\n",
        "\n",
        "    return df.withColumn(new_col_name, to_date(concat(col(year_col),\n",
        "                                                        lit(\"-\"),\n",
        "                                                        col(month_col),\n",
        "                                                        lit(\"-\"),\n",
        "                                                        col(day_col)),\n",
        "                                                \"yyyy-MM-dd\"))\n",
        "\n",
        "df = create_date_column(df, \"start_year\", \"start_month\", \"start_day\", \"start_date\")\n",
        "df = create_date_column(df, \"end_year\", \"end_month\", \"end_day\", \"end_date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtfrS3KIP76E",
        "outputId": "d889b048-1e56-4bcc-b3b1-dccbd8ff4a2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- historic: string (nullable = true)\n",
            " |-- disaster_group: string (nullable = true)\n",
            " |-- disaster_subgroup: string (nullable = true)\n",
            " |-- disaster_type: string (nullable = true)\n",
            " |-- disaster_subtype: string (nullable = true)\n",
            " |-- iso: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- subregion: string (nullable = true)\n",
            " |-- region: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- magnitude: double (nullable = true)\n",
            " |-- start_year: integer (nullable = true)\n",
            " |-- start_month: integer (nullable = true)\n",
            " |-- start_day: integer (nullable = true)\n",
            " |-- end_year: integer (nullable = true)\n",
            " |-- end_month: integer (nullable = true)\n",
            " |-- end_day: integer (nullable = true)\n",
            " |-- total_deaths: integer (nullable = false)\n",
            " |-- total_affected: integer (nullable = false)\n",
            " |-- insured_damage_adjusted: integer (nullable = false)\n",
            " |-- start_date: date (nullable = true)\n",
            " |-- end_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Dropna\n",
        "columns_to_dropna = [\"location\", \"start_date\",\"magnitude\"]\n",
        "df = df.dropna(subset=columns_to_dropna)\n",
        "\n",
        "#Fillna\n",
        "fill_values = {\n",
        "    #\"event_name\": \"None\",\n",
        "    #\"magnitude\": 0,\n",
        "    \"total_deaths\":0,\n",
        "    \"total_affected\":0,\n",
        "    \"insured_damage_adjusted\":0\n",
        "}\n",
        "for column, value in fill_values.items():\n",
        "    df = df.fillna({column: value})\n",
        "\n",
        "#df_indexado = df.filter(~((col(\"disaster_type\").isin(\"Drought\", \"Earthquake\", \"Epidemic\",\n",
        "#                                                                 \"Extreme temperature\", \"Flood\",\n",
        "#                                                                 \"Storm\", \"Wildfire\")) & col(\"magnitude\").isNull()))\n",
        "\n",
        "# Mostrar el esquema final del DataFrame\n",
        "df = df.withColumn(\"end_month\", col(\"end_month\").cast(\"integer\"))\n",
        "df = df.withColumn(\"start_month\", col(\"start_month\").cast(\"integer\"))\n",
        "df = df.withColumn(\"end_day\", col(\"end_day\").cast(\"integer\"))\n",
        "df = df.withColumn(\"start_day\", col(\"start_day\").cast(\"integer\"))\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5QFdSDQP76E",
        "outputId": "43039809-6355-49c4-f079-cca4cbbc184c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['historic', 'disaster_group', 'disaster_subgroup', 'disaster_type', 'disaster_subtype', 'iso', 'country', 'subregion', 'region', 'location', 'magnitude', 'start_year', 'start_month', 'start_day', 'end_year', 'end_month', 'end_day', 'total_deaths', 'total_affected', 'insured_damage_adjusted', 'start_date', 'end_date']\n"
          ]
        }
      ],
      "source": [
        "column_names = df.columns\n",
        "print(column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTIdMCavP76E",
        "outputId": "09ffbc92-81d0-4407-f84b-2e1d532f8220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- magnitude: double (nullable = true)\n",
            " |-- start_year: integer (nullable = true)\n",
            " |-- start_month: integer (nullable = true)\n",
            " |-- start_day: integer (nullable = true)\n",
            " |-- end_year: integer (nullable = true)\n",
            " |-- end_month: integer (nullable = true)\n",
            " |-- end_day: integer (nullable = true)\n",
            " |-- total_deaths: integer (nullable = false)\n",
            " |-- total_affected: integer (nullable = false)\n",
            " |-- insured_damage_adjusted: integer (nullable = false)\n",
            " |-- start_date: date (nullable = true)\n",
            " |-- end_date: date (nullable = true)\n",
            " |-- historic_indexado: integer (nullable = true)\n",
            " |-- disaster_group_indexado: integer (nullable = true)\n",
            " |-- disaster_subgroup_indexado: integer (nullable = true)\n",
            " |-- disaster_type_indexado: integer (nullable = true)\n",
            " |-- disaster_subtype_indexado: integer (nullable = true)\n",
            " |-- iso_indexado: integer (nullable = true)\n",
            " |-- country_indexado: integer (nullable = true)\n",
            " |-- subregion_indexado: integer (nullable = true)\n",
            " |-- region_indexado: integer (nullable = true)\n",
            " |-- location_indexado: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType\n",
        "import pandas as pd\n",
        "\n",
        "columns_to_index = [\"historic\", \"disaster_group\", \"disaster_subgroup\", \"disaster_type\", \"disaster_subtype\",\n",
        "                    \"iso\", \"country\", \"subregion\", \"region\", \"location\"]\n",
        "\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_indexado\", handleInvalid='skip') for col in columns_to_index]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "model = pipeline.fit(df)\n",
        "df_indexado = model.transform(df)\n",
        "\n",
        "# Ajustar índices y eliminar columnas originales\n",
        "for col_name in columns_to_index:\n",
        "    index_col = f\"{col_name}_indexado\"\n",
        "    if index_col in df_indexado.columns:\n",
        "        # Crear nueva columna que ajusta el índice\n",
        "        df_indexado = df_indexado.withColumn(index_col, col(index_col) + 1)\n",
        "        # Eliminar la columna original si es necesario\n",
        "        df_indexado = df_indexado.drop(col_name)\n",
        "\n",
        "# Convertir a enteros\n",
        "for col_name in df_indexado.columns:\n",
        "    if \"_indexado\" in col_name:\n",
        "        df_indexado = df_indexado.withColumn(col_name, col(col_name).cast(IntegerType()))\n",
        "\n",
        "df_indexado.printSchema()\n",
        "\n",
        "glosario = []\n",
        "for indexer in indexers:\n",
        "    input_col = indexer.getInputCol()\n",
        "    output_col = indexer.getOutputCol()\n",
        "    index_labels = model.stages[columns_to_index.index(input_col)].labels\n",
        "\n",
        "    for idx, label in enumerate(index_labels):\n",
        "        glosario.append((output_col, idx + 1, label))  # Sumar 1 al índice\n",
        "\n",
        "glos = pd.DataFrame(glosario, columns=['Columna', 'Índice', 'Definición'])\n",
        "glos.to_excel('glosario.xlsx', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "ZliFi_8Z5OaE"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = [\"start_date\", \"end_date\"]\n",
        "df_indexado = df_indexado.drop(*columns_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to8Z4rNO50-3",
        "outputId": "f77e753a-933e-426b-e55d-d5171545b998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- magnitude: double (nullable = true)\n",
            " |-- start_year: integer (nullable = true)\n",
            " |-- start_month: integer (nullable = true)\n",
            " |-- start_day: integer (nullable = true)\n",
            " |-- end_year: integer (nullable = true)\n",
            " |-- end_month: integer (nullable = true)\n",
            " |-- end_day: integer (nullable = true)\n",
            " |-- total_deaths: integer (nullable = false)\n",
            " |-- total_affected: integer (nullable = false)\n",
            " |-- insured_damage_adjusted: integer (nullable = false)\n",
            " |-- historic_indexado: integer (nullable = true)\n",
            " |-- disaster_group_indexado: integer (nullable = true)\n",
            " |-- disaster_subgroup_indexado: integer (nullable = true)\n",
            " |-- disaster_type_indexado: integer (nullable = true)\n",
            " |-- disaster_subtype_indexado: integer (nullable = true)\n",
            " |-- iso_indexado: integer (nullable = true)\n",
            " |-- country_indexado: integer (nullable = true)\n",
            " |-- subregion_indexado: integer (nullable = true)\n",
            " |-- region_indexado: integer (nullable = true)\n",
            " |-- location_indexado: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_indexado.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dplQOJFmPd6x"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, ChiSqSelector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "VLysA1aUP50a"
      },
      "outputs": [],
      "source": [
        "df_indexado = df_indexado.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_indexado_pd = df_indexado.toPandas()\n",
        "df_indexado_pd.to_excel('df_indexado_2.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLS39zVcz9rI",
        "outputId": "0d56553e-c8f7-4d19-dede-564a01c226cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+\n",
            "|magnitude|start_year|start_month|start_day|end_year|end_month|end_day|total_deaths|total_affected|insured_damage_adjusted|historic_indexado|disaster_group_indexado|disaster_subgroup_indexado|disaster_type_indexado|disaster_subtype_indexado|iso_indexado|country_indexado|subregion_indexado|region_indexado|location_indexado|\n",
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+\n",
            "|    220.0|      1900|          9|        8|    1900|        9|      8|        6000|             0|                      0|                2|                      1|                         3|                     3|                        3|           2|               2|                 6|              2|             1525|\n",
            "|      5.9|      1900|          7|       12|    1900|        7|     12|         140|             0|                      0|                2|                      1|                         2|                     2|                        1|           9|               9|                 8|              1|             2024|\n",
            "|      7.9|      1901|          8|       10|    1901|        8|     10|          18|            24|                      0|                2|                      1|                         2|                     2|                       12|           7|               7|                 3|              1|              108|\n",
            "|      6.4|      1902|         12|       16|    1902|       12|     16|        4562|        125112|                      0|                2|                      1|                         2|                     2|                        1|          35|              35|                 9|              3|              338|\n",
            "|      7.5|      1902|          4|       18|    1902|        4|     18|        2000|             0|                      0|                2|                      1|                         2|                     2|                        1|          53|              53|                 1|              2|             3344|\n",
            "|      6.9|      1902|          2|       13|    1902|        2|     13|          86|         17540|                      0|                2|                      1|                         2|                     2|                        1|          35|              35|                 9|              3|             3587|\n",
            "|      7.7|      1902|          8|       22|    1902|        8|     22|        2500|             0|                      0|                2|                      1|                         2|                     2|                        1|           1|               1|                 3|              1|             4245|\n",
            "|      6.3|      1903|          4|       29|    1903|        4|     29|        3560|         60000|                      0|                2|                      1|                         2|                     2|                        1|           9|               9|                 8|              1|             2554|\n",
            "|      5.7|      1903|          3|       30|    1903|        3|     30|          20|             0|                      0|                2|                      1|                         2|                     2|                        1|         147|             148|                 8|              1|             2804|\n",
            "|      5.8|      1903|          5|       28|    1903|        5|     28|        1000|             0|                      0|                2|                      1|                         2|                     2|                        1|           9|               9|                 8|              1|             4084|\n",
            "|      6.5|      1903|          9|       25|    1903|        9|     25|         350|             0|                      0|                2|                      1|                         2|                     2|                        1|           6|               6|                 2|              1|             2109|\n",
            "|      6.0|      1904|          4|       24|    1904|        4|     24|           3|           208|                      0|                2|                      1|                         2|                     2|                        1|          21|              21|                 3|              1|               24|\n",
            "|      6.2|      1904|          8|       11|    1904|        8|     11|           4|          1620|                      0|                2|                      1|                         2|                     2|                        1|          20|              20|                 7|              3|             3492|\n",
            "|      6.3|      1904|         11|        6|    1904|       11|      6|         145|          2141|                      0|                2|                      1|                         2|                     2|                        1|          21|              21|                 3|              1|               24|\n",
            "|      7.1|      1905|          9|        8|    1905|        9|      8|         557|         22100|                      0|                2|                      1|                         2|                     2|                        1|          18|              18|                 7|              3|             2502|\n",
            "|      7.8|      1905|          4|        4|    1905|        4|      4|       20000|             0|                      0|                2|                      1|                         2|                     2|                        1|           5|               5|                 2|              1|             2074|\n",
            "|      7.0|      1905|          7|        7|    1905|        7|      7|          41|             0|                      0|                2|                      1|                         2|                     2|                       12|           7|               7|                 3|              1|             1502|\n",
            "|      6.6|      1905|          6|        1|    1905|        6|      1|         120|             0|                      0|                2|                      1|                         2|                     2|                        1|          40|              40|                 7|              3|             3636|\n",
            "|      7.8|      1905|          6|        2|    1905|        6|      2|          11|           177|                      0|                2|                      1|                         2|                     2|                        1|           7|               7|                 3|              1|              217|\n",
            "|      5.0|      1905|         11|        8|    1905|       11|      8|           0|             0|                      0|                2|                      1|                         2|                     2|                        1|           1|               1|                 3|              1|                3|\n",
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_indexado_2 = df_indexado.limit(100)\n",
        "df_indexado_2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COz3D_H9P76F",
        "outputId": "3b0b8aa2-fdc9-4027-9552-5fe559bc2627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+\n",
            "|features               |\n",
            "+-----------------------+\n",
            "|[220.0,1.0,3.0,3.0,2.0]|\n",
            "|[5.9,1.0,2.0,1.0,1.0]  |\n",
            "|[7.9,1.0,2.0,12.0,1.0] |\n",
            "|[6.4,1.0,2.0,1.0,3.0]  |\n",
            "|[7.5,1.0,2.0,1.0,2.0]  |\n",
            "|[6.9,1.0,2.0,1.0,3.0]  |\n",
            "|[7.7,1.0,2.0,1.0,1.0]  |\n",
            "|[6.3,1.0,2.0,1.0,1.0]  |\n",
            "|[5.7,1.0,2.0,1.0,1.0]  |\n",
            "|[5.8,1.0,2.0,1.0,1.0]  |\n",
            "+-----------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputCols = [\n",
        "    #'event_name',\n",
        "    'magnitude',\n",
        "    #'magnitude_scale',\n",
        "    #'start_year',\n",
        "    #'start_month',\n",
        "    #'start_day',\n",
        "    #'start_date',\n",
        "    #'end_year',\n",
        "    #'end_month',\n",
        "    #'end_day',\n",
        "    #'end_date',\n",
        "    #'total_deaths',\n",
        "    #'total_affected',\n",
        "    #'insured_damage_adjusted',\n",
        "    #'historic_indexado',\n",
        "    'disaster_group_indexado',\n",
        "    ##'disaster_subgroup_indexado',\n",
        "    'disaster_type_indexado',\n",
        "    'disaster_subtype_indexado',\n",
        "    #'iso_indexado',\n",
        "    ##'country_indexado',\n",
        "    #subregion_indexado',\n",
        "    'region_indexado',\n",
        "    #'location_indexado'\n",
        "]\n",
        "assembler = VectorAssembler(inputCols = inputCols, outputCol = \"features\")\n",
        "df_with_features = assembler.transform(df_indexado_2)\n",
        "\n",
        "df_with_features.select('features').show(10, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c_l1xLB4vM1",
        "outputId": "22e665f7-cd60-45fc-c5f6-cfeed38ed264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- magnitude: double (nullable = true)\n",
            " |-- start_year: integer (nullable = true)\n",
            " |-- start_month: integer (nullable = true)\n",
            " |-- start_day: integer (nullable = true)\n",
            " |-- end_year: integer (nullable = true)\n",
            " |-- end_month: integer (nullable = true)\n",
            " |-- end_day: integer (nullable = true)\n",
            " |-- total_deaths: integer (nullable = false)\n",
            " |-- total_affected: integer (nullable = false)\n",
            " |-- insured_damage_adjusted: integer (nullable = false)\n",
            " |-- historic_indexado: integer (nullable = true)\n",
            " |-- disaster_group_indexado: integer (nullable = true)\n",
            " |-- disaster_subgroup_indexado: integer (nullable = true)\n",
            " |-- disaster_type_indexado: integer (nullable = true)\n",
            " |-- disaster_subtype_indexado: integer (nullable = true)\n",
            " |-- iso_indexado: integer (nullable = true)\n",
            " |-- country_indexado: integer (nullable = true)\n",
            " |-- subregion_indexado: integer (nullable = true)\n",
            " |-- region_indexado: integer (nullable = true)\n",
            " |-- location_indexado: integer (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_with_features.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZbH8cJgP76F",
        "outputId": "3ed88e7b-3643-4c04-bd61-8c02a4969313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+\n",
            "|magnitude|start_year|start_month|start_day|end_year|end_month|end_day|total_deaths|total_affected|insured_damage_adjusted|historic_indexado|disaster_group_indexado|disaster_subgroup_indexado|disaster_type_indexado|disaster_subtype_indexado|iso_indexado|country_indexado|subregion_indexado|region_indexado|location_indexado|            features|\n",
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+\n",
            "|    220.0|      1900|          9|        8|    1900|        9|      8|        6000|             0|                      0|                2|                      1|                         3|                     3|                        3|           2|               2|                 6|              2|             1525|[220.0,1.0,3.0,3....|\n",
            "|      5.9|      1900|          7|       12|    1900|        7|     12|         140|             0|                      0|                2|                      1|                         2|                     2|                        1|           9|               9|                 8|              1|             2024|[5.9,1.0,2.0,1.0,...|\n",
            "|      7.9|      1901|          8|       10|    1901|        8|     10|          18|            24|                      0|                2|                      1|                         2|                     2|                       12|           7|               7|                 3|              1|              108|[7.9,1.0,2.0,12.0...|\n",
            "|      6.4|      1902|         12|       16|    1902|       12|     16|        4562|        125112|                      0|                2|                      1|                         2|                     2|                        1|          35|              35|                 9|              3|              338|[6.4,1.0,2.0,1.0,...|\n",
            "|      7.5|      1902|          4|       18|    1902|        4|     18|        2000|             0|                      0|                2|                      1|                         2|                     2|                        1|          53|              53|                 1|              2|             3344|[7.5,1.0,2.0,1.0,...|\n",
            "|      6.9|      1902|          2|       13|    1902|        2|     13|          86|         17540|                      0|                2|                      1|                         2|                     2|                        1|          35|              35|                 9|              3|             3587|[6.9,1.0,2.0,1.0,...|\n",
            "|      7.7|      1902|          8|       22|    1902|        8|     22|        2500|             0|                      0|                2|                      1|                         2|                     2|                        1|           1|               1|                 3|              1|             4245|[7.7,1.0,2.0,1.0,...|\n",
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+\n",
            "only showing top 7 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_with_features.show(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdOQBunhP76F",
        "outputId": "2639be49-865f-4a8f-85d2-33ad580c3ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+-----------------+\n",
            "|magnitude|start_year|start_month|start_day|end_year|end_month|end_day|total_deaths|total_affected|insured_damage_adjusted|historic_indexado|disaster_group_indexado|disaster_subgroup_indexado|disaster_type_indexado|disaster_subtype_indexado|iso_indexado|country_indexado|subregion_indexado|region_indexado|location_indexado|            features|selected_features|\n",
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+-----------------+\n",
            "|    220.0|      1900|          9|        8|    1900|        9|      8|        6000|             0|                      0|                2|                      1|                         3|                     3|                        3|           2|               2|                 6|              2|             1525|[220.0,1.0,3.0,3....|  [220.0,3.0,3.0]|\n",
            "|      5.9|      1900|          7|       12|    1900|        7|     12|         140|             0|                      0|                2|                      1|                         2|                     2|                        1|           9|               9|                 8|              1|             2024|[5.9,1.0,2.0,1.0,...|    [5.9,2.0,1.0]|\n",
            "|      7.9|      1901|          8|       10|    1901|        8|     10|          18|            24|                      0|                2|                      1|                         2|                     2|                       12|           7|               7|                 3|              1|              108|[7.9,1.0,2.0,12.0...|   [7.9,2.0,12.0]|\n",
            "|      6.4|      1902|         12|       16|    1902|       12|     16|        4562|        125112|                      0|                2|                      1|                         2|                     2|                        1|          35|              35|                 9|              3|              338|[6.4,1.0,2.0,1.0,...|    [6.4,2.0,1.0]|\n",
            "|      7.5|      1902|          4|       18|    1902|        4|     18|        2000|             0|                      0|                2|                      1|                         2|                     2|                        1|          53|              53|                 1|              2|             3344|[7.5,1.0,2.0,1.0,...|    [7.5,2.0,1.0]|\n",
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "selector = ChiSqSelector(numTopFeatures = 3, featuresCol = \"features\", labelCol=\"total_affected\",\n",
        "                         outputCol=\"selected_features\")\n",
        "df_sel = selector.fit(df_with_features).transform(df_with_features)\n",
        "df_sel.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "i-cMtaDJQkcy"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "G97hH1-hR1Hs"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "df_escalado = scaler.fit(df_with_features).transform(df_with_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrHMbkBMR4QL",
        "outputId": "70f541c7-238c-4db1-f85e-47668a8fbcd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+--------------------+\n",
            "|magnitude|start_year|start_month|start_day|end_year|end_month|end_day|total_deaths|total_affected|insured_damage_adjusted|historic_indexado|disaster_group_indexado|disaster_subgroup_indexado|disaster_type_indexado|disaster_subtype_indexado|iso_indexado|country_indexado|subregion_indexado|region_indexado|location_indexado|            features|     features_scaled|\n",
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+--------------------+\n",
            "|    220.0|      1900|          9|        8|    1900|        9|      8|        6000|             0|                      0|                2|                      1|                         3|                     3|                        3|           2|               2|                 6|              2|             1525|[220.0,1.0,3.0,3....|[0.00186105638256...|\n",
            "|      5.9|      1900|          7|       12|    1900|        7|     12|         140|             0|                      0|                2|                      1|                         2|                     2|                        1|           9|               9|                 8|              1|             2024|[5.9,1.0,2.0,1.0,...|[4.99101484415577...|\n",
            "|      7.9|      1901|          8|       10|    1901|        8|     10|          18|            24|                      0|                2|                      1|                         2|                     2|                       12|           7|               7|                 3|              1|              108|[7.9,1.0,2.0,12.0...|[6.68288428285265...|\n",
            "|      6.4|      1902|         12|       16|    1902|       12|     16|        4562|        125112|                      0|                2|                      1|                         2|                     2|                        1|          35|              35|                 9|              3|              338|[6.4,1.0,2.0,1.0,...|[5.41398220382999...|\n",
            "|      7.5|      1902|          4|       18|    1902|        4|     18|        2000|             0|                      0|                2|                      1|                         2|                     2|                        1|          53|              53|                 1|              2|             3344|[7.5,1.0,2.0,1.0,...|[6.34451039511327...|\n",
            "|      6.9|      1902|          2|       13|    1902|        2|     13|          86|         17540|                      0|                2|                      1|                         2|                     2|                        1|          35|              35|                 9|              3|             3587|[6.9,1.0,2.0,1.0,...|[5.83694956350421...|\n",
            "|      7.7|      1902|          8|       22|    1902|        8|     22|        2500|             0|                      0|                2|                      1|                         2|                     2|                        1|           1|               1|                 3|              1|             4245|[7.7,1.0,2.0,1.0,...|[6.51369733898296...|\n",
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+--------------------+\n",
            "only showing top 7 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_escalado.show(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tpwt4my1R9g3"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "f-IiYWy_STcY"
      },
      "outputs": [],
      "source": [
        "# Dividir datos en entrenamiento y prueba\n",
        "train, test = df_escalado.randomSplit([0.7, 0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "navVOqkQxrpK",
        "outputId": "40093fea-8a02-4874-910d-3a66df1ea274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+--------------------+\n",
            "|magnitude|start_year|start_month|start_day|end_year|end_month|end_day|total_deaths|total_affected|insured_damage_adjusted|historic_indexado|disaster_group_indexado|disaster_subgroup_indexado|disaster_type_indexado|disaster_subtype_indexado|iso_indexado|country_indexado|subregion_indexado|region_indexado|location_indexado|            features|     features_scaled|\n",
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+--------------------+\n",
            "|      3.0|      1928|          8|        4|    1928|        8|      4|         128|             0|                      0|                2|                      1|                         2|                     2|                       12|           3|               3|                 4|              1|             1464|[3.0,1.0,2.0,12.0...|[2.31670408555928...|\n",
            "|      4.9|      1914|          5|        8|    1914|        5|      8|         120|           669|                      0|                2|                      1|                         2|                     2|                        1|          18|              18|                 7|              3|              955|[4.9,1.0,2.0,1.0,...|[3.78395000641350...|\n",
            "|      5.3|      1923|         12|       14|    1923|       12|     14|         300|             0|                      0|                2|                      1|                         2|                     2|                        1|          24|              24|                 1|              2|             1893|[5.3,1.0,2.0,1.0,...|[4.09284388448807...|\n",
            "|      5.5|      1925|         12|       14|    1925|       12|     14|         500|             0|                      0|                2|                      1|                         2|                     2|                        1|           6|               6|                 2|              1|              549|[5.5,1.0,2.0,1.0,...|[4.24729082352536...|\n",
            "|      5.7|      1903|          3|       30|    1903|        3|     30|          20|             0|                      0|                2|                      1|                         2|                     2|                        1|         147|             148|                 8|              1|             2804|[5.7,1.0,2.0,1.0,...|[4.40173776256265...|\n",
            "|      5.7|      1923|          5|       26|    1923|        5|     26|        2200|             0|                      0|                2|                      1|                         2|                     2|                        1|           6|               6|                 2|              1|             4042|[5.7,1.0,2.0,1.0,...|[4.40173776256265...|\n",
            "|      5.8|      1903|          5|       28|    1903|        5|     28|        1000|             0|                      0|                2|                      1|                         2|                     2|                        1|           9|               9|                 8|              1|             4084|[5.8,1.0,2.0,1.0,...|[4.47896123208129...|\n",
            "|      5.8|      1920|          9|        7|    1920|        9|      7|        1400|             0|                      0|                2|                      1|                         2|                     2|                        1|          18|              18|                 7|              3|              946|[5.8,1.0,2.0,1.0,...|[4.47896123208129...|\n",
            "|      5.8|      1925|          1|        9|    1925|        1|      9|         200|             0|                      0|                2|                      1|                         2|                     2|                        1|           9|               9|                 8|              1|              401|[5.8,1.0,2.0,1.0,...|[4.47896123208129...|\n",
            "|      5.8|      1925|         10|       15|    1925|       10|     15|          12|          1500|                      0|                2|                      1|                         2|                     2|                        1|           1|               1|                 3|              1|                1|[5.8,1.0,2.0,1.0,...|[4.47896123208129...|\n",
            "|      5.9|      1900|          7|       12|    1900|        7|     12|         140|             0|                      0|                2|                      1|                         2|                     2|                        1|           9|               9|                 8|              1|             2024|[5.9,1.0,2.0,1.0,...|[4.55618470159993...|\n",
            "|      6.0|      1917|         12|       25|    1917|       12|     25|        2650|             0|                      0|                2|                      1|                         2|                     2|                        1|          53|              53|                 1|              2|             1659|[6.0,1.0,2.0,1.0,...|[4.63340817111857...|\n",
            "|      6.0|      1920|         12|       17|    1920|       12|     17|         400|             0|                      0|                2|                      1|                         2|                     2|                        1|          30|              30|                 1|              2|             2648|[6.0,1.0,2.0,1.0,...|[4.63340817111857...|\n",
            "|      6.0|      1930|          5|       14|    1930|        5|     14|          42|             0|                      0|                2|                      1|                         2|                     2|                        1|           1|               1|                 3|              1|                1|[6.0,1.0,2.0,1.0,...|[4.63340817111857...|\n",
            "|      6.2|      1904|          8|       11|    1904|        8|     11|           4|          1620|                      0|                2|                      1|                         2|                     2|                        1|          20|              20|                 7|              3|             3492|[6.2,1.0,2.0,1.0,...|[4.78785511015586...|\n",
            "|      6.2|      1915|          6|       22|    1915|        6|     22|           6|             0|                      0|                2|                      1|                         2|                     2|                        1|           2|               2|                 6|              2|             1384|[6.2,1.0,2.0,1.0,...|[4.78785511015586...|\n",
            "|      6.2|      1920|         11|       26|    1920|       11|     26|         200|             0|                      0|                2|                      1|                         2|                     2|                        1|          40|              40|                 7|              3|             3914|[6.2,1.0,2.0,1.0,...|[4.78785511015586...|\n",
            "|      6.3|      1903|          4|       29|    1903|        4|     29|        3560|         60000|                      0|                2|                      1|                         2|                     2|                        1|           9|               9|                 8|              1|             2554|[6.3,1.0,2.0,1.0,...|[4.86507857967450...|\n",
            "|      6.3|      1913|         11|        4|    1913|       11|      4|         150|             0|                      0|                2|                      1|                         2|                     2|                        1|          15|              15|                 1|              2|              248|[6.3,1.0,2.0,1.0,...|[4.86507857967450...|\n",
            "|      6.3|      1914|         11|       27|    1914|       11|     27|          14|             0|                      0|                2|                      1|                         2|                     2|                        1|          20|              20|                 7|              3|             2400|[6.3,1.0,2.0,1.0,...|[4.86507857967450...|\n",
            "+---------+----------+-----------+---------+--------+---------+-------+------------+--------------+-----------------------+-----------------+-----------------------+--------------------------+----------------------+-------------------------+------------+----------------+------------------+---------------+-----------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "89RyEDqWXJhy"
      },
      "outputs": [],
      "source": [
        "#from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol=\"features_scaled\", labelCol=\"total_affected\")\n",
        "\n",
        "model = lr.fit(train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B65hwFy017Ei",
        "outputId": "12a5bd70-af12-49fc-a831-74dd3c284659"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5625"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluar el modelo\n",
        "predictions = model.transform(test)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"total_affected\", predictionCol=\"prediction\",\n",
        "                                              metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+--------------+\n",
            "|        prediction|total_affected|\n",
            "+------------------+--------------+\n",
            "|122259.42973250139|            11|\n",
            "| 451270.1557320559|             0|\n",
            "|122286.26966148498|           263|\n",
            "|122286.26966148498|         31500|\n",
            "|122287.94715704652|             0|\n",
            "|122292.97964373091|             0|\n",
            "| 451285.2531921092|       1839888|\n",
            "| 122303.0446170998|             0|\n",
            "| 122303.0446170998|          2220|\n",
            "| 451291.9631743551|             0|\n",
            "|122304.72211266134|             0|\n",
            "|122304.72211266134|             0|\n",
            "|122304.72211266134|             0|\n",
            "| 780282.5592271718|             0|\n",
            "|451295.31816547806|         25475|\n",
            "|  451298.673156601|             0|\n",
            "|122311.43209490727|             0|\n",
            "|122311.43209490727|             0|\n",
            "|122314.78708603024|             0|\n",
            "|122326.52955496055|             0|\n",
            "+------------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Load and prepare the dataset\n",
        "data = df_indexado#, header=True, inferSchema=True\n",
        "feature_columns = ['magnitude', 'disaster_group_indexado', 'region_indexado']  # assuming these are the features\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_data, test_data = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train the model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_affected\")\n",
        "model = lr.fit(train_data)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test_data)\n",
        "predictions.select(\"prediction\", \"total_affected\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o7397.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2251.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2251.0 (TID 1321) (DESKTOP-QOP6DNJ executor driver): java.lang.RuntimeException: Labels MUST be in [0, 100), but got 669.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregate_max_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_1$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:891)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:891)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1022)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1021)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:360)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:388)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:360)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4218)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4208)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3423)\r\n\tat org.apache.spark.ml.util.DatasetUtils$.getNumClasses(DatasetUtils.scala:193)\r\n\tat org.apache.spark.ml.classification.Classifier.getNumClasses(Classifier.scala:75)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:144)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.RuntimeException: Labels MUST be in [0, 100), but got 669.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregate_max_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_1$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:891)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:891)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[163], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     16\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_affected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[0;32m     20\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
            "File \u001b[1;32mC:\\Spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
            "File \u001b[1;32mC:\\Spark\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
            "File \u001b[1;32mC:\\Spark\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mC:\\Spark\\python\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o7397.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2251.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2251.0 (TID 1321) (DESKTOP-QOP6DNJ executor driver): java.lang.RuntimeException: Labels MUST be in [0, 100), but got 669.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregate_max_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_1$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:891)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:891)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1022)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1021)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:360)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:388)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:360)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4218)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4208)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4206)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3202)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3423)\r\n\tat org.apache.spark.ml.util.DatasetUtils$.getNumClasses(DatasetUtils.scala:193)\r\n\tat org.apache.spark.ml.classification.Classifier.getNumClasses(Classifier.scala:75)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:144)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.RuntimeException: Labels MUST be in [0, 100), but got 669.0\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregate_max_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_1$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:891)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:891)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = df_indexado_2\n",
        "\n",
        "# Assemble features\n",
        "feature_columns = ['magnitude', 'disaster_group_indexado']\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "data = assembler.transform(data)\n",
        "\n",
        "# Split the data\n",
        "train_data, test_data = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train the model\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"total_affected\")\n",
        "model = rf.fit(train_data)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.transform(test_data)\n",
        "predictions.select(\"prediction\", \"total_affected\").show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
